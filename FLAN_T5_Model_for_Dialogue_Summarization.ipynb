{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/exp_2023_onward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL80dMDL9Gzd",
        "outputId": "9d0400e3-7192-4f9d-ff7f-931a49278f61"
      },
      "id": "sL80dMDL9Gzd",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/exp_2023_onward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Overview**:\n",
        "This project is dedicated to enhancing the FlanT5 model for dialogue summarization. We aim to achieve this through a two-fold approach: firstly, by fine-tuning its parameters effectively, and secondly, by implementing the Reinforcement Learning for Hate Speech Filtering (RLHF) framework to ensure the generated summaries are free from toxic content. Our primary objective is the refinement of the FLAN-T5 model.\n",
        "\n",
        "Key Steps:\n",
        "1. **Dataset Acquisition and Preparation**\n",
        "   - Our project kicks off with the acquisition of the dataset, followed by meticulous preparation for the subsequent training phase.\n",
        "\n",
        "2. **Model Initialization and Efficient Parameter Fine-Tuning**\n",
        "   - The project's next phase entails initializing the model and optimizing it for streamlined parameter fine-tuning.\n",
        "\n",
        "3. **Model Evaluation using Rouge Score**\n",
        "   - Following efficient parameter fine-tuning, its performance is evaluated using the Rouge score, which measures the quality of the generated summaries.\n",
        "\n",
        "4. **Toxicity Assessment of Generated Summaries**\n",
        "   - We employ the 'roberta-hate-speech' model to assess the toxicity of the generated summaries, ensuring they meet non-toxicity standards.\n",
        "\n",
        "5. **Perform Fine-Tuning using RLHF Framework to Detoxify the Summaries**\n",
        "   - The RLHF fine-tuning process involves the following components:\n",
        "     - PPO Model: This model, which undergoes optimized fine-tuning, aims to improve non-toxicity in the summaries.\n",
        "     - Reference Model: A frozen model used to calculate the KL-divergence from the initial model state. It provides an additional reward signal during PPO training to prevent significant deviations from the original Language Model (LLM).\n",
        "     - Score Generator Model: This model is responsible for generating non-toxicity scores for the summaries.\n",
        "\n",
        "6. **Evaluation of Non-Toxicity Performance Enhancement**\n",
        "   - Finally, we assess the improvements in non-toxicity achieved by the RLHF fine-tuned model to ensure the generated summaries meet high standards of safety and quality."
      ],
      "metadata": {
        "id": "zhW7oWLWpDYq"
      },
      "id": "zhW7oWLWpDYq"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f9d24e86-f76f-4a44-90ef-0777752075a8",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9d24e86-f76f-4a44-90ef-0777752075a8",
        "outputId": "33398f83-3a25-4078-9719-6a695ef0ea83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
            "Collecting accelerate (from peft)\n",
            "  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: safetensors, dill, responses, multiprocess, huggingface-hub, tokenizers, accelerate, transformers, datasets, peft, evaluate\n",
            "Successfully installed accelerate-0.24.0 datasets-2.14.6 dill-0.3.7 evaluate-0.4.1 huggingface-hub-0.17.3 multiprocess-0.70.15 peft-0.5.0 responses-0.18.0 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting git+https://github.com/lvwerra/trl.git@25fa1bd\n",
            "  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-mase63ba\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-mase63ba\n",
            "\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running command git checkout -q 25fa1bd\n",
            "  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (4.34.1)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (1.23.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (0.24.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl==0.4.2.dev0) (2.14.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl==0.4.2.dev0) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.17.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl==0.4.2.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.2.dev0) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl==0.4.2.dev0) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.2.dev0) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl==0.4.2.dev0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.4.2.dev0) (1.16.0)\n",
            "Building wheels for collected packages: trl\n",
            "  Building wheel for trl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trl: filename=trl-0.4.2.dev0-py3-none-any.whl size=67533 sha256=c984f7a86408f92ebf042b04147b5524d7445034162c0d68c6e8f7b89a595844\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-meji8mie/wheels/24/b4/20/2fa3a1e47c0411c39e198029315e3af2a2c1d59132913f136f\n",
            "Successfully built trl\n",
            "Installing collected packages: trl\n",
            "Successfully installed trl-0.4.2.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft datasets evaluate\n",
        "!pip install git+https://github.com/lvwerra/trl.git@25fa1bd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d8c20bed-6a30-4847-a507-02969ecb4465",
      "metadata": {
        "tags": [],
        "id": "d8c20bed-6a30-4847-a507-02969ecb4465"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "import torch\n",
        "import evaluate\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5f97d4-ea5f-4072-b5d6-785d1d833ed4",
      "metadata": {
        "tags": [],
        "id": "4a5f97d4-ea5f-4072-b5d6-785d1d833ed4"
      },
      "source": [
        "### **Section 1: Load Dataset and pre-process for Finetuning**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b058b52b-ec4d-4426-8d71-91e898f727f6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b058b52b-ec4d-4426-8d71-91e898f727f6",
        "outputId": "0354c001-99c8-436e-9ac5-d6f9ec52d7fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 12460\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
              "        num_rows: 1500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "dataset_original = load_dataset(huggingface_dataset_name)\n",
        "dataset_original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "51469abe-4d72-4093-a6c6-8e04e19f09eb",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51469abe-4d72-4093-a6c6-8e04e19f09eb",
        "outputId": "95a1d8c8-5c74-4b4c-91bc-f93fb87f7af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
            "        num_rows: 9964\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'labels'],\n",
            "        num_rows: 2492\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def preprocess_and_split_dataset(tokenizer_name,\n",
        "                                 dataset_to_load,\n",
        "                                 min_length,\n",
        "                                 ):\n",
        "\n",
        "    # Load the dataset, using only the \"train\" split for this task.\n",
        "    dataset = load_dataset(dataset_to_load, split=\"train\")\n",
        "\n",
        "    # Filter dialogues with lengths within the specified range.\n",
        "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > min_length,batched=False)\n",
        "\n",
        "    # Initialize the tokenizer, automatically switching between GPU and CPU.\n",
        "\n",
        "    def tokenize_function(example):\n",
        "      start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "      end_prompt = '\\n\\nSummary: '\n",
        "      prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "      example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "      example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "      #example[\"query\"] = tokenizer.decode(example[\"input_ids\"][0])\n",
        "      return example\n",
        "\n",
        "\n",
        "    # Tokenize each dialogue in the dataset.\n",
        "    dataset = dataset.map(tokenize_function, batched=True)\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    # Split the dataset into train and test sections.\n",
        "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
        "\n",
        "    return dataset_splits\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
        "\n",
        "dataset_name = huggingface_dataset_name  # Replace with the actual dataset name\n",
        "min_length = 200\n",
        "max_length = 1000\n",
        "\n",
        "dataset = preprocess_and_split_dataset(tokenizer,\n",
        "                                       dataset_to_load=dataset_name,\n",
        "                                       min_length=min_length)\n",
        "\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 2: Model Initialization and Efficient Parameter Fine-Tuning.**\n"
      ],
      "metadata": {
        "id": "d9qoGTeJuCQ8"
      },
      "id": "d9qoGTeJuCQ8"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "peft_model = get_peft_model(original_model,\n",
        "                            lora_config)\n"
      ],
      "metadata": {
        "id": "V9fcjZCjm5Bq"
      },
      "id": "V9fcjZCjm5Bq",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        ")\n",
        "\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset['test']\n",
        ")"
      ],
      "metadata": {
        "id": "I8EjBJUrnJnC"
      },
      "id": "I8EjBJUrnJnC",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "SRHETJUlmv52",
        "outputId": "a4954dd6-0b08-4fe9-84b0-0a2e48a1d999"
      },
      "id": "SRHETJUlmv52",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2491' max='2491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2491/2491 49:30, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.248700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.187800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.170600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
              " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
              " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 3: Model Evaluation using Rouge Score**"
      ],
      "metadata": {
        "id": "GiwMnbSryQ9A"
      },
      "id": "GiwMnbSryQ9A"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a1a94b14-b375-45e7-9e49-a7f2c341b4ff",
      "metadata": {
        "tags": [],
        "id": "a1a94b14-b375-45e7-9e49-a7f2c341b4ff"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\",\n",
        "                                                        torch_dtype=torch.bfloat16,\n",
        "                                                        device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       'peft-dialogue-summary-checkpoint-local',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False,\n",
        "                                       )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "baseline_human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids.to(original_model.device), generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids.to(peft_model.device), generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}\\n')\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}\\n')\n",
        "print(f'PEFT MODEL: {peft_model_text_output}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvwlfWbf-NTd",
        "outputId": "63182c58-510e-496a-bd91-f8ad7bf0daef"
      },
      "id": "GvwlfWbf-NTd",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASELINE HUMAN SUMMARY:\n",
            "#Person2# shows #Person1# the way to the post office.\n",
            "\n",
            "ORIGINAL MODEL:\n",
            "#Person1# tells #Person1# where the post office is on the Fifth Avenue.\n",
            "\n",
            "PEFT MODEL: #Person1# asks #Person2# where the post office is on Fifth Avenue. #Person1# tells #Person2# to walk two blocks ahead, then turn left.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    prompt = f\"\"\"\n",
        "              Summarize the following conversation.\n",
        "\n",
        "              {dialogue}\n",
        "\n",
        "              Summary: \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "\n",
        "    original_model_outputs = original_model.generate(input_ids=input_ids.to(original_model.device), generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    peft_model_outputs = peft_model.generate(input_ids=input_ids.to(peft_model.device), generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries','original_model_summaries', 'peft_model_summaries'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "LqVcHWcBtX9g",
        "outputId": "96d7bb92-2826-4cd6-b16e-2763579e5c82"
      },
      "id": "LqVcHWcBtX9g",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            human_baseline_summaries  \\\n",
              "0  #Person1# and #Person2# are talking about #Per...   \n",
              "1  #Person2# is doing business through the Intern...   \n",
              "2  #Person1# has to miss school next week and ass...   \n",
              "3  #Person1# and #Person2# will meet Conrad at a ...   \n",
              "4  Maria worries that her mother won't allow her ...   \n",
              "\n",
              "                            original_model_summaries  \\\n",
              "0  #Person1# tells the story of Ricky Martin's si...   \n",
              "1  #Person1# wants to do business through Interne...   \n",
              "2  #Person1# has to miss school next week. #Perso...   \n",
              "3  #Person1# and #Person2# are going to pick Conr...   \n",
              "4  Maria's mother won't allow her to accept a par...   \n",
              "\n",
              "                                peft_model_summaries  \n",
              "0  #Person1# was happy that his favorite singer, ...  \n",
              "1  #Person2# is doing business through Internet n...  \n",
              "2  #Person1# has to miss school next week. #Perso...  \n",
              "3  #Person1# and #Person2# are going to meet Conr...  \n",
              "4  Maria's mother won't allow her to accept a par...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7e93b3ba-606b-46f7-bce3-6a704a4112af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>human_baseline_summaries</th>\n",
              "      <th>original_model_summaries</th>\n",
              "      <th>peft_model_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#Person1# and #Person2# are talking about #Per...</td>\n",
              "      <td>#Person1# tells the story of Ricky Martin's si...</td>\n",
              "      <td>#Person1# was happy that his favorite singer, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#Person2# is doing business through the Intern...</td>\n",
              "      <td>#Person1# wants to do business through Interne...</td>\n",
              "      <td>#Person2# is doing business through Internet n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#Person1# has to miss school next week and ass...</td>\n",
              "      <td>#Person1# has to miss school next week. #Perso...</td>\n",
              "      <td>#Person1# has to miss school next week. #Perso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#Person1# and #Person2# will meet Conrad at a ...</td>\n",
              "      <td>#Person1# and #Person2# are going to pick Conr...</td>\n",
              "      <td>#Person1# and #Person2# are going to meet Conr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Maria worries that her mother won't allow her ...</td>\n",
              "      <td>Maria's mother won't allow her to accept a par...</td>\n",
              "      <td>Maria's mother won't allow her to accept a par...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e93b3ba-606b-46f7-bce3-6a704a4112af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7e93b3ba-606b-46f7-bce3-6a704a4112af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7e93b3ba-606b-46f7-bce3-6a704a4112af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-79a9ce49-a34f-4a08-910f-b27d6c3ed7ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79a9ce49-a34f-4a08-910f-b27d6c3ed7ce')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-79a9ce49-a34f-4a08-910f-b27d6c3ed7ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh60jEaYOez8",
        "outputId": "809b500f-d8a4-4950-aaf0-3d9c98d3b530"
      },
      "id": "mh60jEaYOez8",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-_0mfr1tjj_",
        "outputId": "c1505cd0-3486-45ed-bfb2-4813b3568949"
      },
      "id": "L-_0mfr1tjj_",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL MODEL:\n",
            "{'rouge1': 0.34123586883765394, 'rouge2': 0.1440017083888651, 'rougeL': 0.2831497924525362, 'rougeLsum': 0.2807635096977867}\n",
            "PEFT MODEL:\n",
            "{'rouge1': 0.4007367787331477, 'rouge2': 0.18594748933648575, 'rougeL': 0.32906372654516264, 'rougeLsum': 0.333225720958218}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 4: Toxicity Assessment of Generated Summaries**\n"
      ],
      "metadata": {
        "id": "1g-8Tfn8yihR"
      },
      "id": "1g-8Tfn8yihR"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7f038a9f-e04b-49bc-8923-8ef3816919ee",
      "metadata": {
        "tags": [],
        "id": "7f038a9f-e04b-49bc-8923-8ef3816919ee"
      },
      "outputs": [],
      "source": [
        "class ToxicModel:\n",
        "    def __init__(self,toxicity_model_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
        "        self.model  = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
        "\n",
        "    def __call__(self, text,return_probs=False):\n",
        "        input_ids = self.tokenizer(text, padding=True, truncation=True,return_tensors=\"pt\").input_ids\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids.to(self.model.device)).logits\n",
        "\n",
        "        if return_probs:\n",
        "            probabilities = logits.softmax(dim=-1).tolist()\n",
        "            return probabilities\n",
        "        else:\n",
        "            not_hate_index = 0\n",
        "            score= logits[:, not_hate_index].tolist()\n",
        "            return score\n",
        "\n",
        "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "toxicity_evaluator = ToxicModel(toxicity_model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f4e6a05-2398-4ca7-a176-d5a1ff27fe39",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f4e6a05-2398-4ca7-a176-d5a1ff27fe39",
        "outputId": "a7c53db2-bf6d-47ef-b4fb-928919ff1cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non toxic example: \n",
            "Non toxic Probablity [[0.9963293671607971, 0.0036706167738884687]]\n",
            "Non toxic Reward [3.114100694656372]\n",
            "Toxic example: \n",
            "Toxic Probablity [[0.2564719617366791, 0.7435280084609985]]\n",
            "Toxic Reward [-0.6921164393424988]\n"
          ]
        }
      ],
      "source": [
        "print(\"Non toxic example: \")\n",
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "print(\"Non toxic Probablity\",toxicity_evaluator(non_toxic_text,return_probs=True))\n",
        "print(\"Non toxic Reward\",toxicity_evaluator(non_toxic_text,return_probs=False))\n",
        "\n",
        "print(\"Toxic example: \")\n",
        "toxic_text= \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "print(\"Toxic Probablity\",toxicity_evaluator(toxic_text,return_probs=True))\n",
        "print(\"Toxic Reward\",toxicity_evaluator(toxic_text,return_probs=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "316ab128-33ff-4a1e-8936-47bfa29d48a3",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "316ab128-33ff-4a1e-8936-47bfa29d48a3",
        "outputId": "6630f68e-4a85-4b53-baae-b14d96a6cf98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3it [00:03,  1.06s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
            "11it [00:15,  1.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxicity [mean, std] before detox: [2.3352706134319305, 1.390401880705651]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_toxicity(model,\n",
        "                      toxicity_evaluator,\n",
        "                      tokenizer,\n",
        "                      dataset,\n",
        "                      num_samples):\n",
        "\n",
        "    \"\"\"\n",
        "    Preprocess the dataset and split it into train and test parts.\n",
        "\n",
        "    Parameters:\n",
        "    - model (trl model): Model to be evaluated.\n",
        "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
        "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
        "    - dataset (dataset): Input dataset for the evaluation.\n",
        "    - num_samples (int): Maximum number of samples for the evaluation.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two numpy.float64 values:\n",
        "    - mean (numpy.float64): Mean of the samples toxicity.\n",
        "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
        "    \"\"\"\n",
        "\n",
        "    max_new_tokens=100\n",
        "\n",
        "    toxicities = []\n",
        "    input_texts = []\n",
        "    for i, sample in tqdm(enumerate(dataset)):\n",
        "        start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "        end_prompt = '\\n\\nSummary: '\n",
        "        input_text = start_prompt + sample[\"dialogue\"] + end_prompt\n",
        "\n",
        "        if i > num_samples:\n",
        "            break\n",
        "\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
        "                                             top_k=0.0,\n",
        "                                             top_p=1.0,\n",
        "                                             do_sample=True)\n",
        "        try:\n",
        "          input_ids=input_ids.to(model.device)\n",
        "        except:\n",
        "          input_ids=input_ids.to(model.current_device)\n",
        "        response_token_ids = model.generate(input_ids=input_ids,\n",
        "                                            generation_config=generation_config)\n",
        "\n",
        "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        toxicity_score = toxicity_evaluator(generated_text)\n",
        "\n",
        "        toxicities.extend(toxicity_score)\n",
        "\n",
        "    # Compute mean & std using np.\n",
        "    mean = np.mean(toxicities)\n",
        "    std = np.std(toxicities)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
        "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=peft_model,\n",
        "                                                                          toxicity_evaluator=toxicity_evaluator,\n",
        "                                                                          tokenizer=tokenizer,\n",
        "                                                                          dataset=dataset[\"test\"],\n",
        "                                                                          num_samples=10)\n",
        "\n",
        "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba81c90-1ac8-4403-ac1a-d4c75c6df4f0",
      "metadata": {
        "id": "1ba81c90-1ac8-4403-ac1a-d4c75c6df4f0"
      },
      "source": [
        "### **Section 5: Perform Fine-Tuning using RLHF Framework to Detoxify the Summaries.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cac21fb-fea5-4e80-a741-87f35ae72c62",
      "metadata": {
        "id": "0cac21fb-fea5-4e80-a741-87f35ae72c62"
      },
      "source": [
        "The fine-tuning loop consists of the following main steps:\n",
        "1. Get the query responses from the policy LLM (PEFT model).\n",
        "2. Get sentiments for query/responses from hate speech RoBERTa model.\n",
        "3. Optimize policy with PPO using the (query, response, reward) triplet.\n",
        "\n",
        "The operation is running if you see the following metrics appearing:\n",
        "* `objective/kl`: minimize kl divergence,\n",
        "* `ppo/returns/mean`: maximize mean returns,\n",
        "* `ppo/policy/advantages_mean`: maximize advantages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1e86bab0-6dee-4dff-a754-b584ed962723",
      "metadata": {
        "tags": [],
        "id": "1e86bab0-6dee-4dff-a754-b584ed962723"
      },
      "outputs": [],
      "source": [
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
        "                                                               torch_dtype=torch.bfloat16,\n",
        "                                                               is_trainable=True,\n",
        "                                                               )\n",
        "\n",
        "ref_model = create_reference_model(ppo_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "494e09a1-9024-4f38-91eb-d73cdc3239e6",
      "metadata": {
        "tags": [],
        "id": "494e09a1-9024-4f38-91eb-d73cdc3239e6"
      },
      "outputs": [],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=4\n",
        "batch_size=16\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=model_name,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=dataset[\"train\"],\n",
        "                         data_collator=collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4bc55397-92b8-4f61-9ec2-c8b39d5f8962",
      "metadata": {
        "tags": [],
        "id": "4bc55397-92b8-4f61-9ec2-c8b39d5f8962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f21b18-f7f8-4da1-d8b7-4abf4864b9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:25, 25.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 27.78833770751953\n",
            "ppo/returns/mean: -0.7307109832763672\n",
            "ppo/policy/advantages_mean: -1.1751219997080398e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2it [00:51, 25.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 32.53301239013672\n",
            "ppo/returns/mean: -0.9490570425987244\n",
            "ppo/policy/advantages_mean: 5.699219762789198e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3it [01:19, 26.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 30.954715728759766\n",
            "ppo/returns/mean: -0.8603411912918091\n",
            "ppo/policy/advantages_mean: 7.568892179676823e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r4it [01:47, 27.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 31.311710357666016\n",
            "ppo/returns/mean: -0.7944016456604004\n",
            "ppo/policy/advantages_mean: 2.4990960412196728e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r5it [02:19, 28.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 36.272239685058594\n",
            "ppo/returns/mean: -0.8264066576957703\n",
            "ppo/policy/advantages_mean: -9.53857615115794e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6it [02:46, 28.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 35.81541442871094\n",
            "ppo/returns/mean: -0.9980186223983765\n",
            "ppo/policy/advantages_mean: 1.4282910854035435e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r7it [03:16, 28.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 38.20220947265625\n",
            "ppo/returns/mean: -1.0372788906097412\n",
            "ppo/policy/advantages_mean: -3.279390270449767e-08\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r8it [03:47, 29.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 41.561058044433594\n",
            "ppo/returns/mean: -1.2687175273895264\n",
            "ppo/policy/advantages_mean: 3.6715286366728606e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r9it [04:19, 30.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 33.37474060058594\n",
            "ppo/returns/mean: -0.9183297157287598\n",
            "ppo/policy/advantages_mean: 4.912426909697842e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [04:42, 28.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "objective/kl: 26.23990249633789\n",
            "ppo/returns/mean: -0.7045215368270874\n",
            "ppo/policy/advantages_mean: 6.8878613923573084e-09\n",
            "---------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "output_min_length = 100\n",
        "output_max_length = 400\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "max_ppo_steps = 10\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Break when you reach max_steps.\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Get response from FLAN-T5/PEFT LLM.\n",
        "    summary_tensors = []\n",
        "\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        max_new_tokens = output_length_sampler()\n",
        "\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # This needs to be called \"response\".\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    # Compute reward outputs.\n",
        "    rewards = toxicity_evaluator(batch[\"response\"])\n",
        "    rewards=[torch.tensor(r) for r in rewards]\n",
        "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
        "\n",
        "    # Run PPO step.\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)\n",
        "\n",
        "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7903f5df-a9de-41eb-b239-38bc367b5654",
      "metadata": {
        "id": "7903f5df-a9de-41eb-b239-38bc367b5654"
      },
      "source": [
        "### **Section 6:Evaluation of Non-Toxicity Performance Enhancement**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3b093d43-6197-4cc0-b933-29030479a7d0",
      "metadata": {
        "tags": [],
        "id": "3b093d43-6197-4cc0-b933-29030479a7d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a82efb6-1c17-430f-ccb1-6d69ee5f4372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11it [00:19,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxicity [mean, std] after detox: [2.2445485591888428, 1.1239272173089407]\n",
            "Percentage improvement of toxicity score after detoxification:\n",
            "mean: 3.88%\n",
            "std: 19.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model,\n",
        "                                                                        toxicity_evaluator=toxicity_evaluator,\n",
        "                                                                        tokenizer=tokenizer,\n",
        "                                                                        dataset=dataset[\"test\"],\n",
        "                                                                        num_samples=10)\n",
        "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')\n",
        "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
        "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
        "\n",
        "print(f'Percentage improvement of toxicity score after detoxification:')\n",
        "print(f'mean: {mean_improvement*100:.2f}%')\n",
        "print(f'std: {std_improvement*100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "instance_type": "ml.m5.2xlarge",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}